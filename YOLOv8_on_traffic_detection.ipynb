{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7149559,"sourceType":"datasetVersion","datasetId":4066836}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/binfeng2021/computer-vision-yolov8-on-traffic-detection?scriptVersionId=167680207\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Project Overview\n\nIn today's digitally driven world, computer vision stands as a cornerstone of technological innovation, offering machines the ability to perceive and interpret visual information much like the human eye. From autonomous vehicles navigating bustling city streets to facial recognition systems securing our smartphones, the applications of computer vision are ubiquitous and far-reaching.\n\nAmong the myriad algorithms powering computer vision systems, one standout is YOLOâ€”You Only Look Once. YOLO represents a paradigm shift in object detection, offering unparalleled speed and accuracy by simultaneously predicting bounding boxes and class probabilities for multiple objects within a single pass through the neural network. In simple terms, YOLO enables machines to swiftly identify and categorize objects in images or video streams with remarkable efficiency.\n\nNow, imagine leveraging the capabilities of YOLO to tackle a pressing real-world challenge: traffic sign detection. In a world where road safety is paramount, accurately identifying and interpreting traffic signs is crucial for ensuring smooth traffic flow and preventing accidents. This is where our project comes into play.\n\n### Main Objectives\n\n* Check the datasets\n* Understand Model YOLOv8\n* Train the YOLOv8 model and analyze the results\n* Test the final model","metadata":{}},{"cell_type":"markdown","source":"### Import all necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-18T20:11:41.367573Z","iopub.execute_input":"2024-03-18T20:11:41.368117Z","iopub.status.idle":"2024-03-18T20:11:53.820974Z","shell.execute_reply.started":"2024-03-18T20:11:41.367998Z","shell.execute_reply":"2024-03-18T20:11:53.819982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import all necessary libraries\nimport numpy as np \nimport pandas as pd \nimport os\nimport random\nfrom PIL import Image\nimport cv2\nfrom IPython.display import Video\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = 'darkgrid')\nimport pathlib\nimport glob\nfrom tqdm.notebook import trange, tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom ultralytics import YOLO","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-18T20:11:53.822866Z","iopub.execute_input":"2024-03-18T20:11:53.823177Z","iopub.status.idle":"2024-03-18T20:11:53.831761Z","shell.execute_reply.started":"2024-03-18T20:11:53.823148Z","shell.execute_reply":"2024-03-18T20:11:53.830908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trun off wandb reporting for this notebook\nos.environ['WANDB_DISABLED'] = 'true'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore the datasets\n\n### Check the training images","metadata":{}},{"cell_type":"code","source":"train_img_dir = '/kaggle/input/cardetection/train/images'","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:11:53.832831Z","iopub.execute_input":"2024-03-18T20:11:53.833105Z","iopub.status.idle":"2024-03-18T20:11:53.842443Z","shell.execute_reply.started":"2024-03-18T20:11:53.833072Z","shell.execute_reply":"2024-03-18T20:11:53.841603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_samples = 9\nimage_files = os.listdir(train_img_dir)\nrand_imgs = random.sample(image_files, num_samples)\n\nfig, axes = plt.subplots(3,3, figsize = (11, 11))\nfor i in range(num_samples):\n    image = rand_imgs[i]\n    ax = axes[i // 3, i%3]\n    ax.imshow(plt.imread(os.path.join(train_img_dir, image)))\n    ax.set_title(f'Image {i+1}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:11:53.844993Z","iopub.execute_input":"2024-03-18T20:11:53.845423Z","iopub.status.idle":"2024-03-18T20:11:56.401557Z","shell.execute_reply.started":"2024-03-18T20:11:53.845392Z","shell.execute_reply":"2024-03-18T20:11:56.400614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the **data.yaml** file, we have 15 different labeled classes in the dataset, shown as below:\n\n['Green Light', 'Red Light', 'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110', 'Speed Limit 120', 'Speed Limit 20', 'Speed Limit 30', 'Speed Limit 40', 'Speed Limit 50', 'Speed Limit 60', 'Speed Limit 70', 'Speed Limit 80', 'Speed Limit 90', 'Stop']","metadata":{}},{"cell_type":"markdown","source":"## Understand Model YOLOv8\n\nYOLO, which stands for \"You Only Look Once,\" is a groundbreaking object detection algorithm in computer vision. Unlike traditional object detection algorithms that involve multiple stages of processing, YOLO processes the entire image in a single pass through a convolutional neural network (CNN). This approach allows YOLO to achieve real-time object detection with impressive speed and accuracy.\n\n### Technical Details\n\n1. The algorithm will take the input image and split it into a grid of cells. Each cell will predict bounding boxes and a confidence score for each box. The bounding boxes indicates the location of detected object in the images and the confidence value indicates the model's certainty of the prediction. \n\n2. Other than confidence socre for each bounding boxes, YOLO also predicts an \"objectness\" score for each box that indicate the likelihood that the box contains a meaningful object, not just background clutter.\n\n3. Last but not the least, YOLO also predicts the class for each bounding box detected. The probability generated measures the likelihood of the detected obeject belonging to different predefined classes. \n\n4. To generate final output, YOLO will combine a set of bounding boxes from previous steps, each bounding box is associated with its class label and confidence socre. Those combined bounding boxes then can represent the objects detected along with its location in the image and classified label. To prevent redundant and improve localization accuracy, YOLO applied a technic called Non-Maximum Suppression (NMS) to those predicted bounding boxes. NMS selects the most confident bounding boxes while suppressing overlapping detections with lower confidence scores. \n\n","metadata":{}},{"cell_type":"markdown","source":"### Check out the pre-trained YOLOv8 Model","metadata":{}},{"cell_type":"code","source":"model = YOLO(\"yolov8n.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:11:56.402807Z","iopub.execute_input":"2024-03-18T20:11:56.403124Z","iopub.status.idle":"2024-03-18T20:11:57.141798Z","shell.execute_reply.started":"2024-03-18T20:11:56.403098Z","shell.execute_reply":"2024-03-18T20:11:57.140997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3,3, figsize = (11, 11))\nfor i in range(num_samples):\n    image = rand_imgs[i]\n    ax = axes[i // 3, i%3]\n    result_predict = model.predict(source = os.path.join(train_img_dir, image), imgsz = (416))\n    ax.imshow(result_predict[0].plot())\n    ax.set_title(f'Image {i+1}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:11:57.14324Z","iopub.execute_input":"2024-03-18T20:11:57.143551Z","iopub.status.idle":"2024-03-18T20:12:02.16853Z","shell.execute_reply.started":"2024-03-18T20:11:57.143524Z","shell.execute_reply":"2024-03-18T20:12:02.167422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pre-trained YOLO does not perform very well, most of the traffic signs were not detected, or correctly labled. We will fine-tune the model using the specified dataset and see how the performance might change. ","metadata":{}},{"cell_type":"markdown","source":"## Fine-tune YOLOv8 with given training dataset","metadata":{}},{"cell_type":"code","source":"final_model = YOLO('yolov8n.yaml').load('yolov8m.pt')","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:16:25.10666Z","iopub.execute_input":"2024-03-18T20:16:25.107123Z","iopub.status.idle":"2024-03-18T20:16:25.392108Z","shell.execute_reply.started":"2024-03-18T20:16:25.10708Z","shell.execute_reply":"2024-03-18T20:16:25.391158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Result_Final_model = final_model.train(data=\"/kaggle/input/cardetection/data.yaml\",epochs=100, imgsz = 416, device = 0)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:16:37.093557Z","iopub.execute_input":"2024-03-18T20:16:37.093937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze the final model's performance","metadata":{}},{"cell_type":"code","source":"list_of_metrics = [\"P_curve.png\", \"R_curve.png\", \"PR_curve.png\", \"F1_curve.png\",\"confusion_matrix.png\", \"results.png\"]\nfor i in list_of_metrics:\n    \n    image = cv2.imread(f'/kaggle/working/runs/detect/train/{i}')\n    plt.figure(figsize=(16, 12))\n    plt.imshow(image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:12:03.542943Z","iopub.status.idle":"2024-03-18T20:12:03.543304Z","shell.execute_reply.started":"2024-03-18T20:12:03.54314Z","shell.execute_reply":"2024-03-18T20:12:03.543156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation of the plots\n\n1. **Precision-confidence Curve**: a graphical representation of how precision of the model changes at different confidence levels. In the first plot, we can see that the precisions of the model increases for all classes as the confidence increases. We are able to reach to precision score 1 for all classes when the confidence threshold is 0.958.\n\n2. **Recall-confidence Curve**: a graphical representation of how recall of the model changes at different confidence levels. In the second plot, we can see that the recall of the model decreases for all classes as the confidence increases. The recall for all classes is 0.94 when the confidence threshold is 0.\n\n3. **Precision-Recall Curve**: a graphical representation of the trade-off between precision and recall for different threshold used. From the third plot, we see that the model's precision decreases as the recall increases. When using IoU threshold (intersection over Union) of 0.5, the model is able to achieve mAP (mean average precision) of 0.908.\n\n4. **F1-Confidence Curve**: a graphic representation of how F1 score of the model changes at different confidence levels. Since F1 score is caluclated using both precision and recall scores, it can be a good visualization of how the model is preforming overall. From the fourth plot, we can see that the F1 score increases and then decreases as the confidence threshold increases. When setting the confidence threshold as 0.319, we are able to achieve a F1 score of 0.88 for all classes. \n\n5. **Confusion Matrix**: a table that allows visualization of the performance of a classificaiotn model by summarizing the correct and incorrect classifications. The diagonal of the table shows all ture positives made by the model. As we can see from the fifth plot, most of high value numbers in the table are in the diagonal line, so we can conclude that the model is able to make correct prediction in most cases. \n\n6. **Results plot during training**: a combanation of different metrics measured during training process. In the sixth plot, we can see multiple visualization of different metrics values changes over different epochs. To better understand the results, we need to understand the different loss values measured first. \n\nbox_loss: is also known as localication loss or regression loss. It measures the discrepancy between the predicted bounding box coordinates and the ground truth bounding box coordinates for each object in the image.\n\ncls_loss: is konwn as classification loss. It measures the accuracy of the predicted class labels assigned to each bounding box.\n\ndfl_loss: is known as domain-fused loss. It measures the discrepancy between feature representations learned by the model across different domains. The goal of minimizing dfl loss is to better align the feature representations across different domains, so that the model is able to improve the preformance in real-world scenarios where the testing data may differ from the training data. \n\nFrom the plots included in the sixth figure, we can see that all of those three loss values decreases and the precision and recall scores are increasing as more epoches trained. ","metadata":{}},{"cell_type":"markdown","source":"## Test the model on test data","metadata":{}},{"cell_type":"code","source":"test_img_dir = '/kaggle/input/cardetection/test/images'\nnum_samples = 9\nimage_files = os.listdir(test_img_dir)\ntest_imgs = random.sample(image_files, num_samples)\n\nfig, axes = plt.subplots(3,3, figsize = (11, 11))\nfor i in range(num_samples):\n    image = test_imgs[i]\n    ax = axes[i // 3, i%3]\n    result_predict = final_model.predict(source = os.path.join(test_img_dir, image), imgsz = (416))\n    ax.imshow(result_predict[0].plot())\n    ax.set_title(f'Image {i+1}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T20:12:03.544493Z","iopub.status.idle":"2024-03-18T20:12:03.54482Z","shell.execute_reply.started":"2024-03-18T20:12:03.544659Z","shell.execute_reply":"2024-03-18T20:12:03.544674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the results obtained from the test images, we can see a significant performance increase from the pre-trained model. YOLOv8 has performed very well after training and can detect most objects that we specified in the dataset. ","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this project, we utilized the YOLO model for traffic detection, starting with familiarization and testing of a pre-trained model. Then, we have fine-tuned the model with our dataset. The retrained model shows a significantly improved performance. We have also analyzed different metrics and plots obtained during the training process. All of those plots shows the confirmation of enhanced accuracy. \n\n### Current Limitation & Future work\n\n- To improve the performance further, we could involve further optimization by adjusting parameters used in the YOLO model, for this project, I have only used default parameters mostly, but we can try out different parameter combinations to find the optional model to use. \n\n- During the analysis of the training results, we noticed that the model performance continue to increase in different metrics as number of epoches increase. So we can continue to train the model with more epoches, this should result in performance increase as well.\n\n- Last but not least, the provided dataset only contains ~3.5K training images. If we can expand the training dataset further by including more samples, we could obtain a even better results. \n\n\n### Thank you!\n\nIf you found this notebook interesting, please give me a upvote! If you have any thoughts, I would love to hear it in the comments section! Thank you for reading!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}